{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elenajia/anaconda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numbers\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "tknzr = TweetTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# spacy.en.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 640 There\n",
      "lowercased: 530 there\n",
      "lemma: 530 there\n",
      "shape: 489815 Xxxxx\n",
      "prefix: 2907 T\n",
      "suffix: 48458 ere\n",
      "log probability: -7.347356796264648\n",
      "Brown cluster id: 1918\n",
      "----------------------------------------\n",
      "original: 474 is\n",
      "lowercased: 474 is\n",
      "lemma: 488 be\n",
      "shape: 21581 xx\n",
      "prefix: 570 i\n",
      "suffix: 474 is\n",
      "log probability: -4.457748889923096\n",
      "Brown cluster id: 762\n",
      "----------------------------------------\n",
      "original: 523 an\n",
      "lowercased: 523 an\n",
      "lemma: 523 an\n",
      "shape: 21581 xx\n",
      "prefix: 469 a\n",
      "suffix: 523 an\n",
      "log probability: -6.014852046966553\n",
      "Brown cluster id: 3\n",
      "----------------------------------------\n",
      "There is an art, it says, or rather, a knack to flying.\n",
      "The knack lies in learning how to throw yourself at the ground and miss.\n",
      "In the beginning the Universe was created.\n",
      "This has made a lot of people very angry and been widely regarded as a bad move.\n"
     ]
    }
   ],
   "source": [
    "# spacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp('They told us to duck.')\n",
    "\n",
    "from spacy.en import English\n",
    "parser = English()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### test spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 640 There\n",
      "lowercased: 530 there\n",
      "lemma: 530 there\n",
      "shape: 489815 Xxxxx\n",
      "prefix: 2907 T\n",
      "suffix: 48458 ere\n",
      "log probability: -7.347356796264648\n",
      "Brown cluster id: 1918\n",
      "----------------------------------------\n",
      "original: 474 is\n",
      "lowercased: 474 is\n",
      "lemma: 488 be\n",
      "shape: 21581 xx\n",
      "prefix: 570 i\n",
      "suffix: 474 is\n",
      "log probability: -4.457748889923096\n",
      "Brown cluster id: 762\n",
      "----------------------------------------\n",
      "original: 523 an\n",
      "lowercased: 523 an\n",
      "lemma: 523 an\n",
      "shape: 21581 xx\n",
      "prefix: 469 a\n",
      "suffix: 523 an\n",
      "log probability: -6.014852046966553\n",
      "Brown cluster id: 3\n",
      "----------------------------------------\n",
      "There is an art, it says, or rather, a knack to flying.\n",
      "The knack lies in learning how to throw yourself at the ground and miss.\n",
      "In the beginning the Universe was created.\n",
      "This has made a lot of people very angry and been widely regarded as a bad move.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "multiSentence = \"There is an art, it says, or rather, a knack to flying.\" \\\n",
    "                 \"The knack lies in learning how to throw yourself at the ground and miss.\" \\\n",
    "                 \"In the beginning the Universe was created. This has made a lot of people \"\\\n",
    "                 \"very angry and been widely regarded as a bad move.\"\n",
    "\n",
    "# all you have to do to parse text is this:\n",
    "#note: the first time you run spaCy in a file it takes a little while to load up its modules\n",
    "parsedData = parser(multiSentence)\n",
    "\n",
    "# Let's look at the tokens\n",
    "# All you have to do is iterate through the parsedData\n",
    "# Each token is an object with lots of different properties\n",
    "# A property with an underscore at the end returns the string representation\n",
    "# while a property without the underscore returns an index (int) into spaCy's vocabulary\n",
    "# The probability estimate is based on counts from a 3 billion word\n",
    "# corpus, smoothed using the Simple Good-Turing method.\n",
    "for i, token in enumerate(parsedData):\n",
    "    print(\"original:\", token.orth, token.orth_)\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    print(\"----------------------------------------\")\n",
    "    if i > 1:\n",
    "        break\n",
    "# Let's look at the sentences\n",
    "sents = []\n",
    "# the \"sents\" property returns spans\n",
    "# spans have indices into the original string\n",
    "# where each index value represents a token\n",
    "for span in parsedData.sents:\n",
    "    # go from the start to the end of each span, returning each token in the sentence\n",
    "    # combine each token using join()\n",
    "    sent = ''.join(parsedData[i].string for i in range(span.start, span.end)).strip()\n",
    "    sents.append(sent)\n",
    "\n",
    "for sentence in sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There ADV\n",
      "is VERB\n",
      "an DET\n",
      "art NOUN\n",
      ", PUNCT\n",
      "it PRON\n",
      "says VERB\n",
      ", PUNCT\n",
      "or CONJ\n",
      "rather ADV\n",
      ", PUNCT\n",
      "a DET\n",
      "knack NOUN\n",
      "to ADP\n",
      "flying NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the part of speech tags of the first sentence\n",
    "for span in parsedData.sents:\n",
    "    sent = [parsedData[i] for i in range(span.start, span.end)]\n",
    "    break\n",
    "\n",
    "for token in sent:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "... S -> NP VP\n",
    "... PP -> P NP\n",
    "... NP -> Det N | Det N PP | 'I'\n",
    "... VP -> V NP | VP PP\n",
    "... Det -> 'an' | 'my'\n",
    "... N -> 'elephant' | 'pajamas'\n",
    "... V -> 'shot'\n",
    "... P -> 'in'\n",
    "... \"\"\")\n",
    "\n",
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det boy [] []\n",
      "boy nsubj ran ['The'] ['with']\n",
      "with prep boy [] ['dog']\n",
      "the det dog [] []\n",
      "spotted amod dog [] []\n",
      "dog pobj with ['the', 'spotted'] []\n",
      "quickly advmod ran [] []\n",
      "ran ROOT ran ['boy', 'quickly'] ['after', '.']\n",
      "after prep ran [] ['firetruck']\n",
      "the det firetruck [] []\n",
      "firetruck pobj after ['the'] []\n",
      ". punct ran [] []\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the dependencies of this example:\n",
    "example = \"The boy with the spotted dog quickly ran after the firetruck.\"\n",
    "parsedEx = parser(example)\n",
    "# shown as: original token, dependency tag, head word, left dependents, right dependents\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Mary) (VP (V saw) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "\n",
    "sent = \"Mary saw Bob\".split()\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "for tree in rd_parser.parse(sent):\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for token in \"what is going on\":\n",
    "#    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
